{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7suDgbcG-rg"
      },
      "source": [
        "# Pruning and Sparcity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-0LGrJ3G-ri"
      },
      "source": [
        "Firstly Installing required packages and downloading the datasets and pretrained VGG model. I am using CIFAR 10 dataset and VGG network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "damNcxDYG-ri",
        "outputId": "ed24aa6a-bfee-4124-b26e-3fc4c529b5fa"
      },
      "outputs": [],
      "source": [
        "print('Installing torchprofile...')\n",
        "!pip install torchprofile 1>/dev/null\n",
        "print('All required packages have been successfully installed!')\n",
        "\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import OrderedDict, defaultdict\n",
        "from typing import Union, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torchprofile import profile_macs\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torchprofile import profile_macs\n",
        "\n",
        "#device = 'mps'\n",
        "device = 'cpu'\n",
        "#device = 'cuda'\n",
        "''' setting seeds to ensure reproducibility '''\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eso5hxo5G-rj"
      },
      "source": [
        "Defining a misc function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfloJ-ikG-rj"
      },
      "outputs": [],
      "source": [
        "def download_url(url, model_dir='.', overwrite=False):\n",
        "    \"\"\"\n",
        "    Download a file from a URL to a specified directory.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the file to download.\n",
        "        model_dir (str, optional): The directory where the file will be saved. Defaults to '.'.\n",
        "        overwrite (bool, optional): If True, overwrite the file if it already exists. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The path to the downloaded file if successful, or None if the download fails.\n",
        "    \"\"\"\n",
        "    import os, sys, ssl  # Import necessary libraries\n",
        "    from urllib.request import urlretrieve  # Import urlretrieve function from urllib.request\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context  # Avoid SSL certificate verification\n",
        "\n",
        "    # Extract the filename from the URL\n",
        "    target_dir = url.split('/')[-1]\n",
        "\n",
        "    # Expand the model directory to an absolute path\n",
        "    model_dir = os.path.expanduser(model_dir)\n",
        "\n",
        "    try:\n",
        "        # Check if the model directory exists, if not, create it\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.makedirs(model_dir)\n",
        "\n",
        "        # Join the model directory with the target directory to get the full path\n",
        "        model_dir = os.path.join(model_dir, target_dir)\n",
        "\n",
        "        # Set the cached file path\n",
        "        cached_file = model_dir\n",
        "\n",
        "        # Check if the file does not exist or if overwrite is True\n",
        "        if not os.path.exists(cached_file) or overwrite:\n",
        "            # Print message indicating that the file is being downloaded\n",
        "            sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
        "\n",
        "            # Download the file from the URL and save it to the cached file path\n",
        "            urlretrieve(url, cached_file)\n",
        "\n",
        "        # Return the path to the downloaded file\n",
        "        return cached_file\n",
        "\n",
        "    except Exception as e:\n",
        "        # If an exception occurs during the download process:\n",
        "        # - Remove any lock file so that download can be attempted again next time.\n",
        "        os.remove(os.path.join(model_dir, 'download.lock'))\n",
        "\n",
        "        # Print an error message indicating the failure\n",
        "        sys.stderr.write('Failed to download from url %s' % url + '\\n' + str(e) + '\\n')\n",
        "\n",
        "        # Return None to indicate failure\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x5-J7RKG-rj"
      },
      "source": [
        "Defining my VGG net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfHGGPiUG-rj"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "    counts = defaultdict(int)\n",
        "\n",
        "    def add(name: str, layer: nn.Module) -> None:\n",
        "      layers.append((f\"{name}{counts[name]}\", layer))\n",
        "      counts[name] += 1\n",
        "\n",
        "    in_channels = 3\n",
        "    for x in self.ARCH:\n",
        "      if x != 'M':\n",
        "        # conv-bn-relu\n",
        "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
        "        add(\"bn\", nn.BatchNorm2d(x))\n",
        "        add(\"relu\", nn.ReLU(True))\n",
        "        in_channels = x\n",
        "      else:\n",
        "        # maxpool\n",
        "        add(\"pool\", nn.MaxPool2d(2))\n",
        "\n",
        "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
        "    self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
        "    x = self.backbone(x)\n",
        "\n",
        "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
        "    x = x.mean([2, 3])\n",
        "\n",
        "    # classifier: [N, 512] => [N, 10]\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbvwmbuTG-rk"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "  model: nn.Module,\n",
        "  dataloader: DataLoader,\n",
        "  criterion: nn.Module,\n",
        "  optimizer: Optimizer,\n",
        "  scheduler: LambdaLR,\n",
        "  callbacks = None\n",
        ") -> None:\n",
        "  model.train()\n",
        "\n",
        "  for inputs, targets in tqdm(dataloader, desc='train', leave=False):\n",
        "    # Move the data from CPU to device\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Reset the gradients (from the last iteration)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward inference\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Calculating loss\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update optimizer and LR scheduler\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if callbacks is not None:\n",
        "        for callback in callbacks:\n",
        "            callback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyDxH-oZG-rk"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(\n",
        "  model: nn.Module,\n",
        "  dataloader: DataLoader,\n",
        "  verbose=True,\n",
        ") -> float:\n",
        "  model.eval()\n",
        "\n",
        "  num_samples = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False,\n",
        "                              disable=not verbose):\n",
        "    # Move the data from CPU to device\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Inference\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Convert logits to class indices\n",
        "    outputs = outputs.argmax(dim=1)\n",
        "\n",
        "    # Update metrics\n",
        "    num_samples += targets.size(0)\n",
        "    num_correct += (outputs == targets).sum()\n",
        "\n",
        "  return (num_correct / num_samples * 100).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq8_W8C_G-rk"
      },
      "source": [
        "Defining helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2atSxIQyG-rk"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def get_model_macs(model, inputs) -> int:\n",
        "    \"\"\"\n",
        "    Get the number of MACs (multiply-accumulate operations) for the given model with the provided inputs.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model.\n",
        "        inputs: The input tensor or tensors to the model.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of MACs.\n",
        "    \"\"\"\n",
        "    return profile_macs(model, inputs)\n",
        "\n",
        "\n",
        "def get_sparsity(tensor: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the sparsity of the given tensor.\n",
        "\n",
        "    Sparsity is defined as the ratio of the number of zeros to the total number of elements.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): The input tensor.\n",
        "\n",
        "    Returns:\n",
        "        float: The sparsity of the tensor.\n",
        "    \"\"\"\n",
        "    return 1 - float(tensor.count_nonzero()) / tensor.numel()\n",
        "\n",
        "\n",
        "def get_model_sparsity(model: nn.Module) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the sparsity of the given model.\n",
        "\n",
        "    Sparsity is defined as the ratio of the number of zeros to the total number of elements across all parameters in the model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model.\n",
        "\n",
        "    Returns:\n",
        "        float: The sparsity of the model.\n",
        "    \"\"\"\n",
        "    num_nonzeros, num_elements = 0, 0\n",
        "    for param in model.parameters():\n",
        "        num_nonzeros += param.count_nonzero()\n",
        "        num_elements += param.numel()\n",
        "    return 1 - float(num_nonzeros) / num_elements\n",
        "\n",
        "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
        "    \"\"\"\n",
        "    Calculate the total number of parameters in the model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model.\n",
        "        count_nonzero_only (bool, optional): If True, count only nonzero weights. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        int: The total number of parameters in the model.\n",
        "    \"\"\"\n",
        "    num_counted_elements = 0\n",
        "    for param in model.parameters():\n",
        "        if count_nonzero_only:\n",
        "            num_counted_elements += param.count_nonzero()\n",
        "        else:\n",
        "            num_counted_elements += param.numel()\n",
        "    return num_counted_elements\n",
        "\n",
        "\n",
        "def get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
        "    \"\"\"\n",
        "    Calculate the model size in bits.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model.\n",
        "        data_width (int, optional): The number of bits per element. Defaults to 32.\n",
        "        count_nonzero_only (bool, optional): If True, count only nonzero weights. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        int: The size of the model in bits.\n",
        "    \"\"\"\n",
        "    return get_num_parameters(model, count_nonzero_only) * data_width\n",
        "\n",
        "# Constants for byte multiples\n",
        "Byte = 8\n",
        "KiB = 1024 * Byte\n",
        "MiB = 1024 * KiB\n",
        "GiB = 1024 * MiB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkHgnGCJG-rl"
      },
      "source": [
        "### Real Shit\n",
        "\n",
        "Loading the pre-trained model and CIFAR 10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqgIsa_pG-rl",
        "outputId": "c9d61642-c191-4f6a-a1d5-2bb91f48a03a"
      },
      "outputs": [],
      "source": [
        "checkpoint_url = \"https://hanlab18.mit.edu/files/course/labs/vgg.cifar.pretrained.pth\"\n",
        "checkpoint = torch.load(download_url(checkpoint_url), map_location=\"cpu\")\n",
        "\n",
        "model = VGG().to(device)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# Define a recovery function so I can play around with the model and still recover it to its original state.\n",
        "recover_model = lambda: model.load_state_dict(checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA4aM2GvG-rl",
        "outputId": "23816eb3-9e3c-4674-9a3c-e16f514d20ab"
      },
      "outputs": [],
      "source": [
        "image_size = 32\n",
        "\n",
        "# Define transformations for training and testing datasets\n",
        "transforms = {\n",
        "    \"train\": Compose([\n",
        "        RandomCrop(image_size, padding=4),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "    ]),\n",
        "    \"test\": ToTensor(),\n",
        "}\n",
        "\n",
        "# Initializing empty dictionary to store datasets\n",
        "dataset = {}\n",
        "\n",
        "# Loop over train and test splits\n",
        "for split in [\"train\", \"test\"]:\n",
        "    # Create CIFAR10 dataset with specified root, train/test split, download if necessary, and apply transformations\n",
        "    dataset[split] = CIFAR10(\n",
        "        root=\"data/cifar10\",\n",
        "        train=(split == \"train\"),  # Set train=True for training split, train=False for test split\n",
        "        download=True,  # Download the dataset if it's not found in the root directory\n",
        "        transform=transforms[split],\n",
        "    )\n",
        "\n",
        "# Initializing empty dictionary to store data loaders\n",
        "dataloader = {}\n",
        "\n",
        "# Loop over train and test splits\n",
        "for split in ['train', 'test']:\n",
        "    # Create DataLoader for the corresponding dataset split\n",
        "    dataloader[split] = DataLoader(\n",
        "        dataset[split],  # Use the dataset for this split\n",
        "        batch_size=512,\n",
        "        shuffle=(split == 'train'),\n",
        "        num_workers=0,\n",
        "        pin_memory=True,  # Pin memory for faster data transfer to device\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNgriZq7G-rl"
      },
      "source": [
        "### Evaluating pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a3a67a053c2a49fe8c72f1ff86b72b5a",
            "82fbfff370d243a0bf35784749087fe3",
            "634dcc6a4d914893a572633b44fa3f85",
            "6248f94183e04f529708684d15cf7f99",
            "4cf6fe1dbbd04f47897b9c1e31a69fac",
            "f3dfb37959a244cda5886f688dd16d02",
            "53ec9c422fb743429de2111a6f583c90",
            "be55c14919ae4b549660190e0aa8739d",
            "e0714e355e4b412abeda0e8d807a525c",
            "8452e36fcfd44a78960c29919c62d1f9",
            "82daccb2efe243b0a3d437dc22da34b5"
          ]
        },
        "id": "ViJ8WHicG-rl",
        "outputId": "abbd41fb-c983-44af-8995-ded324b99819"
      },
      "outputs": [],
      "source": [
        "dense_model_accuracy = evaluate(model, dataloader['test'])\n",
        "dense_model_size = get_model_size(model)\n",
        "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
        "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKM4p7CMG-rl"
      },
      "source": [
        "### Before pruning , lets take a look at the weight distribution in the current model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jadRhC4AG-rl"
      },
      "outputs": [],
      "source": [
        "def plot_weight_distribution(model, bins=256, count_nonzero_only=False):\n",
        "    fig, axes = plt.subplots(3,3, figsize=(10, 6))\n",
        "    axes = axes.ravel()\n",
        "    plot_index = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 1:\n",
        "            ax = axes[plot_index]\n",
        "            if count_nonzero_only:\n",
        "                param_cpu = param.detach().view(-1).cpu()\n",
        "                param_cpu = param_cpu[param_cpu != 0].view(-1)\n",
        "                ax.hist(param_cpu, bins=bins, density=True,\n",
        "                        color = 'black', alpha = 0.5)\n",
        "            else:\n",
        "                ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,color = 'black', alpha = 0.5)\n",
        "                # param_cpu = param.detach().view(-1).cpu()\n",
        "                # total_weights = len(param_cpu)\n",
        "                # param_cpu = param_cpu[(param_cpu <= 0.01) & (param_cpu >= -0.01)].view(-1)\n",
        "                # zero_weights = len(param_cpu)\n",
        "                # print(f'Number of zero parameters in {name} is {zero_weights}/{total_weights}')\n",
        "            ax.set_xlabel(name)\n",
        "            ax.set_ylabel('density')\n",
        "            plot_index += 1\n",
        "        # else :\n",
        "        #     print(f'Skipping {name}')\n",
        "    fig.suptitle('Histogram of Weights')\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(top=0.925)\n",
        "    plt.show()\n",
        "\n",
        "plot_weight_distribution(model)\n",
        "#plot_weight_distribution(model,count_nonzero_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD0VozuxG-rl"
      },
      "source": [
        "The weight distribution plot clearly indicates that a substantial portion of the model's weights are close to zero. This suggests that these weights may not contribute significantly to the model's performance. As a result, there's a significant potential to reduce the model's size by pruning these unnecessary weights. This could lead to improved efficiency and make the model more suitable for deployment on devices with limited resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBKKgEooG-rm"
      },
      "source": [
        "### Prune Time\n",
        "\n",
        "Sparsity = #(zeros)/#(elements) for some weight.\n",
        "\n",
        "To prune a model to some sparsity, find the number of non-zeros (say k) to keep and remove everything below the kth smallest value.\n",
        "\n",
        "Sensitivity Scan = set sparsity to x for some layer and analyze the accuracy of the resultant model, then set sparsity to (say) x + 0.1 and repeat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV5xYHLQG-rm"
      },
      "source": [
        "#### Fine Grained Prune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izz3DqC3G-rm"
      },
      "outputs": [],
      "source": [
        "def fine_grained_prune(tensor: torch.Tensor, sparsity: float) -> torch.Tensor:\n",
        "    # Ensure sparsity is within [0.0, 1.0]\n",
        "    sparsity = min(max(0.0, sparsity), 1.0)\n",
        "\n",
        "    # If sparsity is 1.0, zero out the tensor and return a tensor of zeros\n",
        "    if sparsity == 1.0:\n",
        "        tensor.zero_()\n",
        "        return torch.zeros_like(tensor)\n",
        "    # If sparsity is 0.0, return a tensor of ones\n",
        "    elif sparsity == 0.0:\n",
        "        return torch.ones_like(tensor)\n",
        "\n",
        "    # Calculate the total number of elements in the tensor\n",
        "    num_elements = tensor.numel()\n",
        "\n",
        "    # Calculate the number of zeros to retain based on sparsity\n",
        "    num_zeros = round(sparsity * num_elements)\n",
        "\n",
        "    # Compute the importance of each weight by taking its absolute value\n",
        "    importance = torch.abs(tensor)\n",
        "\n",
        "    # Find the threshold value that separates important weights from non-important ones\n",
        "    threshold = torch.kthvalue(input=importance.flatten(), k=num_zeros).values\n",
        "\n",
        "    # Generate a binary mask where 1 represents important weights and 0 represents non-important ones\n",
        "    mask = torch.gt(importance, threshold)\n",
        "\n",
        "    # Apply the mask to the tensor to prune non-important weights\n",
        "    tensor.mul_(mask)\n",
        "\n",
        "    # Return the binary mask indicating which weights were pruned\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-yyYBaCG-rm"
      },
      "source": [
        "Testing and visualizing the fine_grained_prune method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL4-HU5QG-rm"
      },
      "outputs": [],
      "source": [
        "def plot_matrix(tensor, ax, title):\n",
        "    \"\"\"\n",
        "    Display a grid representing the tensor, where zero values are white and non-zero values are blue.\n",
        "\n",
        "    Args:\n",
        "    - tensor: Input tensor to visualize\n",
        "    - ax: Axis object to plot on\n",
        "    - title: Title for the plot\n",
        "    \"\"\"\n",
        "    ax.imshow(tensor.cpu().numpy() == 0, cmap='tab20c')\n",
        "    ax.set_title(title)\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_xticklabels([])\n",
        "    for i in range(tensor.shape[1]):\n",
        "            for j in range(tensor.shape[0]):\n",
        "                text = ax.text(j, i, f'{tensor[i, j].item():.2f}',\n",
        "                                ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random_tensor = torch.randn(6, 6)\n",
        "fig, axes = plt.subplots(1,2, figsize=(6, 10))\n",
        "ax_left,ax_right = axes.ravel()\n",
        "plot_matrix(random_tensor, ax_left, \"Dense Tensor\")\n",
        "\n",
        "pruned_tensor = random_tensor*fine_grained_prune(tensor=random_tensor,sparsity=0.6)\n",
        "plot_matrix(pruned_tensor, ax_right, \"Sparse Tensor\")\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlJfex4QG-rm"
      },
      "source": [
        "Wrapping fine_grained_prune function into a class for pruning the whole model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI_uxBa0G-rm"
      },
      "outputs": [],
      "source": [
        "class FineGrainedPruner:\n",
        "    \"\"\"\n",
        "    A class for fine-grained pruning of PyTorch model weights based on a given sparsity dictionary.\n",
        "\n",
        "    Attributes:\n",
        "        masks (dict): A dictionary containing masks for pruning model parameters.\n",
        "\n",
        "    Methods:\n",
        "        __init__(model, sparsity_dict):\n",
        "            Initializes the pruner object by computing masks for pruning based on the provided model and sparsity dictionary.\n",
        "\n",
        "        apply(model):\n",
        "            Applies the computed masks to the weights of the given model.\n",
        "\n",
        "        prune(model, sparsity_dict):\n",
        "            Static method to compute masks for pruning based on the provided model and sparsity dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, sparsity_dict):\n",
        "        \"\"\"\n",
        "        Initializes the pruner object by computing masks for pruning based on the provided model and sparsity dictionary.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): The PyTorch model to be pruned.\n",
        "            sparsity_dict (dict): A dictionary containing sparsity levels for different layers of the model.\n",
        "        \"\"\"\n",
        "        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply(self, model):\n",
        "        \"\"\"\n",
        "        Applies the computed masks to the weights of the given model.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): The PyTorch model to which the masks are applied.\n",
        "        \"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in self.masks:\n",
        "                param *= self.masks[name]\n",
        "\n",
        "    @staticmethod\n",
        "    @torch.no_grad()\n",
        "    def prune(model, sparsity_dict):\n",
        "        \"\"\"\n",
        "        Static method to compute masks for pruning based on the provided model and sparsity dictionary.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): The PyTorch model to be pruned.\n",
        "            sparsity_dict (dict): A dictionary containing sparsity levels for different layers of the model.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing masks for pruning model parameters.\n",
        "        \"\"\"\n",
        "        masks = dict()\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.dim() > 1: # we only prune conv and fc weights\n",
        "                masks[name] = fine_grained_prune(param, sparsity_dict[name])\n",
        "        return masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA78mdRJG-rm"
      },
      "source": [
        "### Sensitivity Scan\n",
        "\n",
        "The sensitivity scan is a method used to see how pruning a single layer of a neural network affects its accuracy. We go through each layer of the network and start pruning it and graphing its accuracy.\n",
        "\n",
        "By doing this for each layer and tracking the accuracy as we go, we can figure out which parts of the network are really important for its accuracy and which parts we can prune without causing too much trouble. It's basically about finding the balance between making the network smaller and maintaining accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGYIV5RXG-rm"
      },
      "source": [
        "Defining sensitivity_scan function that will prune every layer with increasing sparsities and measure the accuracy of the model after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnlYu5f3G-rn"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sensitivity_scan(model, dataloader, scan_step=0.05, scan_start=0.3, scan_end=1.0, verbose=True):\n",
        "    sparsities = np.arange(start=scan_start, stop=scan_end, step=scan_step)\n",
        "    accuracies = []\n",
        "    named_conv_weights = [(name, param) for (name, param) \\\n",
        "                          in model.named_parameters() if param.dim() > 1]\n",
        "    for i_layer, (name, param) in enumerate(named_conv_weights):\n",
        "        param_clone = param.detach().clone()\n",
        "        accuracy = []\n",
        "        for sparsity in tqdm(sparsities, desc=f'scanning {i_layer}/{len(named_conv_weights)} weight - {name}'):\n",
        "            fine_grained_prune(param.detach(), sparsity=sparsity)\n",
        "            acc = evaluate(model, dataloader, verbose=False)\n",
        "            if verbose:\n",
        "                print(f'\\r    sparsity={sparsity:.2f}: accuracy={acc:.2f}%', end='')\n",
        "            # restore\n",
        "            param.copy_(param_clone)\n",
        "            accuracy.append(acc)\n",
        "        if verbose:\n",
        "            print(f'\\r    sparsity=[{\",\".join([\"{:.2f}\".format(x) for x in sparsities])}]: accuracy=[{\", \".join([\"{:.2f}%\".format(x) for x in accuracy])}]', end='')\n",
        "        accuracies.append(accuracy)\n",
        "    return sparsities, accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rcu6rrRG-rn"
      },
      "source": [
        "Execute the sensitivity scan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM9r3K5EG-rn"
      },
      "outputs": [],
      "source": [
        "sparsities, accuracies = sensitivity_scan(\n",
        "    model, dataloader['test'], scan_step=0.1, scan_start=0.4, scan_end=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DtrM6eBG-rn"
      },
      "source": [
        "### Plotting Sensitivity Scan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys_Gh99qG-rn"
      },
      "outputs": [],
      "source": [
        "def plot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy):\n",
        "    \"\"\"\n",
        "    Plot the sensitivity scan results showing how accuracy varies with pruning sparsity.\n",
        "\n",
        "    Args:\n",
        "    - sparsities (numpy.ndarray): Array of pruning sparsities.\n",
        "    - accuracies (list of lists): List of accuracy values for each layer at different sparsity levels.\n",
        "    - dense_model_accuracy (float): Accuracy of the original dense model without pruning.\n",
        "    \"\"\"\n",
        "    # Calculate lower bound accuracy to indicate significant performance drop\n",
        "    lower_bound_accuracy = 100 - (100 - dense_model_accuracy) * 1.5\n",
        "\n",
        "    # Create subplots to display sensitivity curves\n",
        "    fig, axes = plt.subplots(3, int(math.ceil(len(accuracies) / 3)), figsize=(15, 8))\n",
        "    axes = axes.ravel()  # Flatten the axes for easier indexing\n",
        "    plot_index = 0\n",
        "\n",
        "    # Iterate over named parameters of the model\n",
        "    for name, param in model.named_parameters():\n",
        "        # Check if parameter dimension is greater than 1 (indicating weight parameters)\n",
        "        if param.dim() > 1:\n",
        "            ax = axes[plot_index]  # Select current axis for plotting\n",
        "\n",
        "            # Plot accuracy curve after pruning for the current layer\n",
        "            curve = ax.plot(sparsities, accuracies[plot_index])\n",
        "\n",
        "            # Plot lower bound accuracy line to indicate significant performance drop\n",
        "            line = ax.plot(sparsities, [lower_bound_accuracy] * len(sparsities))\n",
        "\n",
        "            # Set x-axis ticks and y-axis limits\n",
        "            ax.set_xticks(np.arange(start=0.4, stop=1.0, step=0.1))\n",
        "            ax.set_ylim(80, 95)\n",
        "\n",
        "            # Set title, labels, and legend\n",
        "            ax.set_title(name)\n",
        "            ax.set_xlabel('sparsity')\n",
        "            ax.set_ylabel('top-1 accuracy')\n",
        "            ax.legend([\n",
        "                'accuracy after pruning',\n",
        "                f'{lower_bound_accuracy / dense_model_accuracy * 100:.0f}% of dense model accuracy'\n",
        "            ])\n",
        "\n",
        "            # Add gridlines along the x-axis\n",
        "            ax.grid(axis='x')\n",
        "\n",
        "            plot_index += 1  # Move to the next subplot\n",
        "\n",
        "    # Add title and adjust layout for better spacing\n",
        "    fig.suptitle('Sensitivity Curves: Validation Accuracy vs. Pruning Sparsity')\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(top=0.925)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33TrU61JG-rn"
      },
      "source": [
        "Besides checking how much pruning affects a layer's performance, it's also crucial to know how many parameters are in that layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JZ5jV5YG-rn"
      },
      "outputs": [],
      "source": [
        "def plot_num_parameters_distribution(model):\n",
        "    num_parameters = dict()\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 1:\n",
        "            num_parameters[name] = param.numel()\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    plt.grid(axis='y')\n",
        "    plt.bar(list(num_parameters.keys()), list(num_parameters.values()))\n",
        "    plt.title('#Parameter Distribution')\n",
        "    plt.ylabel('Number of Parameters')\n",
        "    plt.xticks(rotation=60)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_num_parameters_distribution(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6rzArbRG-rn"
      },
      "source": [
        "### Experimentation Time\n",
        "\n",
        "Now that all the tools are defined , its time to define sparsity values ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_cd58TwG-rn"
      },
      "outputs": [],
      "source": [
        "recover_model() #Every time I experiment , I need to recover model to its original state.\n",
        "\n",
        "sparsity_dict = {\n",
        "    'backbone.conv0.weight': 0,\n",
        "    'backbone.conv1.weight': 0.7,\n",
        "    'backbone.conv2.weight': 0.8,\n",
        "    'backbone.conv3.weight': 0.7,\n",
        "    'backbone.conv4.weight': 0.7,\n",
        "    'backbone.conv5.weight': 0.8,\n",
        "    'backbone.conv6.weight': 0.8,\n",
        "    'backbone.conv7.weight': 0.9,\n",
        "    'classifier.weight': 0.95\n",
        "}\n",
        "\n",
        "\n",
        "pruner = FineGrainedPruner(model, sparsity_dict)\n",
        "print(f'After pruning with sparsity dictionary')\n",
        "for name, sparsity in sparsity_dict.items():\n",
        "    print(f'  {name}: {sparsity:.2f}')\n",
        "print(f'The sparsity of each layer becomes')\n",
        "for name, param in model.named_parameters():\n",
        "    if name in sparsity_dict:\n",
        "        print(f'  {name}: {get_sparsity(param):.2f}')\n",
        "\n",
        "sparse_model_size = get_model_size(model, count_nonzero_only=True)\n",
        "print(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\n",
        "sparse_model_accuracy = evaluate(model, dataloader['test'])\n",
        "print(f\"Sparse model has accuracy={sparse_model_accuracy:.2f}% before fintuning\")\n",
        "\n",
        "plot_weight_distribution(model, count_nonzero_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzBfVtnG-rn"
      },
      "source": [
        "### The pruned model's accuracy is pretty low, so we need to fine-tune it just like we normally train a model. However, at each epoch during fine-tuning, we have to apply the pruning mask again, or else the model's size will go back to what it was before pruning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIB4LDNBG-ro"
      },
      "outputs": [],
      "source": [
        "num_finetune_epochs = 4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_sparse_model_checkpoint = dict()\n",
        "best_accuracy = 0\n",
        "print(f'Finetuning Fine-grained Pruned Sparse Model')\n",
        "for epoch in range(num_finetune_epochs):\n",
        "    # At the end of each train iteration, we have to apply the pruning mask\n",
        "    #    to keep the model sparse during the training\n",
        "    train(model, dataloader['train'], criterion, optimizer, scheduler,\n",
        "          callbacks=[lambda: pruner.apply(model)])\n",
        "    accuracy = evaluate(model, dataloader['test'])\n",
        "    is_best = accuracy > best_accuracy\n",
        "    if is_best:\n",
        "        best_sparse_model_checkpoint['state_dict'] = copy.deepcopy(model.state_dict())\n",
        "        best_accuracy = accuracy\n",
        "    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-do1suIG-ro"
      },
      "source": [
        "Lastly , results of the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4c_rfXqG-ro"
      },
      "outputs": [],
      "source": [
        "# load the best sparse model checkpoint to evaluate the final performance\n",
        "model.load_state_dict(best_sparse_model_checkpoint['state_dict'])\n",
        "sparse_model_size = get_model_size(model, count_nonzero_only=True)\n",
        "print(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\n",
        "sparse_model_accuracy = evaluate(model, dataloader['test'])\n",
        "print(f\"Sparse model has accuracy={sparse_model_accuracy:.2f}% after fintuning\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 (clang-1500.1.0.2.5)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cf6fe1dbbd04f47897b9c1e31a69fac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53ec9c422fb743429de2111a6f583c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6248f94183e04f529708684d15cf7f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8452e36fcfd44a78960c29919c62d1f9",
            "placeholder": "​",
            "style": "IPY_MODEL_82daccb2efe243b0a3d437dc22da34b5",
            "value": " 12/20 [03:07&lt;01:57, 14.69s/it]"
          }
        },
        "634dcc6a4d914893a572633b44fa3f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be55c14919ae4b549660190e0aa8739d",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0714e355e4b412abeda0e8d807a525c",
            "value": 12
          }
        },
        "82daccb2efe243b0a3d437dc22da34b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82fbfff370d243a0bf35784749087fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3dfb37959a244cda5886f688dd16d02",
            "placeholder": "​",
            "style": "IPY_MODEL_53ec9c422fb743429de2111a6f583c90",
            "value": "eval:  60%"
          }
        },
        "8452e36fcfd44a78960c29919c62d1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a67a053c2a49fe8c72f1ff86b72b5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82fbfff370d243a0bf35784749087fe3",
              "IPY_MODEL_634dcc6a4d914893a572633b44fa3f85",
              "IPY_MODEL_6248f94183e04f529708684d15cf7f99"
            ],
            "layout": "IPY_MODEL_4cf6fe1dbbd04f47897b9c1e31a69fac"
          }
        },
        "be55c14919ae4b549660190e0aa8739d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0714e355e4b412abeda0e8d807a525c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3dfb37959a244cda5886f688dd16d02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

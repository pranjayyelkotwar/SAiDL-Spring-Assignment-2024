{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rdkit\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "\n",
    "# Load the ESOL dataset\n",
    "data = MoleculeNet(root=\".\", name=\"ESOL\")\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the dataset\n",
    "print(\"Dataset type: \", type(data))\n",
    "print(\"Dataset features: \", data.num_features)\n",
    "print(\"Dataset target: \", data.num_classes)\n",
    "print(\"Dataset length: \", data.len)\n",
    "print(\"Dataset sample: \", data[0])\n",
    "print(\"Sample  nodes: \", data[0].num_nodes)\n",
    "print(\"Sample  edges: \", data[0].num_edges)\n",
    "\n",
    "# edge_index = graph connections\n",
    "# smiles = molecule with its atoms\n",
    "# x = node features (32 nodes have each 9 features)\n",
    "# y = labels (dimension)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Gat Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool , GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "embedding_size = 64\n",
    "\n",
    "def custom_exp(edge_index, attention_scores):\n",
    "    # Extract the number of nodes (N) from the shape of the attention score matrix\n",
    "    N = attention_scores.size(0)\n",
    "    \n",
    "    # Initialize an empty tensor to store the attention coefficients\n",
    "    attention_coefficients = torch.zeros_like(attention_scores)\n",
    "    \n",
    "    # Loop over each source node index (row index of edge_index)\n",
    "    for i in range(edge_index.size(1)):\n",
    "        src_idx, dst_idx = edge_index[:, i]\n",
    "        \n",
    "        numerator = torch.exp(attention_scores[src_idx, dst_idx])\n",
    "\n",
    "        # get an array with attention scores of neighbours of src_idx\n",
    "        neighbours = edge_index[1, edge_index[0] == src_idx]\n",
    "        denominator = torch.sum(torch.exp(attention_scores[src_idx, neighbours]))\n",
    "\n",
    "        attention_coefficients[src_idx, dst_idx] = numerator / denominator\n",
    "\n",
    "    return attention_coefficients\n",
    "\n",
    "\n",
    "class PJzGAT(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PJzGAT, self).__init__(aggr='add')\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_channels, out_channels)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_channels, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(out_channels, out_channels),  # Adjust input channels for MLP\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = torch.matmul(x, self.W)  # Apply linear transformation\n",
    "\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Compute attention coefficients\n",
    "        a_input = torch.cat([h.repeat(1, x.size(0)).view(x.size(0) * x.size(0), -1), h.repeat(x.size(0), 1)], dim=1).view(x.size(0), -1, 2 * self.out_channels)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze())\n",
    "\n",
    "        # Apply mask and activation function\n",
    "        row , col = edge_index\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        mask = torch.zeros_like(e)\n",
    "        mask[edge_index[0], edge_index[1]] = 1\n",
    "        attention = mask * e\n",
    "        attention = F.leaky_relu(attention, negative_slope=0.2)\n",
    "        attention = custom_exp(edge_index, attention)\n",
    "\n",
    "        # Perform message passing with attention\n",
    "        out = self.propagate(edge_index, x=h, attention=attention)\n",
    "\n",
    "        # Optionally apply MLP after aggregation\n",
    "        out = self.mlp(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, attention):\n",
    "        # Compute messages with attention coefficients\n",
    "        attention = attention.view(-1)\n",
    "        attention = attention[attention != 0].view(-1, 1)\n",
    "        buffer = attention.view(-1, 1) * x_j\n",
    "        return buffer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Graph Neural Network\n",
    "\n",
    "Building a Graph Neural Network works the same way as building a Convolutional\n",
    "\n",
    "---\n",
    "\n",
    "Neural Network, we simple add some layers.\n",
    "\n",
    "The GCN simply extends torch.nn.Module.\n",
    "GCNConv expects:\n",
    "- in_channels = Size of each input sample.\n",
    "- out_channels = Size of each output sample.\n",
    "\n",
    "We apply three convolutional layers, which means we learn the information about 3 neighbor hops. After that we apply a pooling layer to combine the information of the individual nodes, as we want to perform graph-level prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init parent\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = PJzGAT(data.num_features, embedding_size)\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = Linear(embedding_size*2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "\n",
    "        hidden = nn.Linear(data.num_features, data.num_features)(x)    \n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Global Pooling (stack different aggregations)\n",
    "        hidden = torch.cat([gmp(hidden, batch_index),\n",
    "                            gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.out(hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "model = GCN()\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Root mean squared error\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Use GPU for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap data in a data loader\n",
    "data_size = len(data)\n",
    "NUM_GRAPHS_PER_BATCH = 512   #initially 64\n",
    "loader = DataLoader(data[:int(data_size * 0.8)],\n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(data[int(data_size * 0.8):],\n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "def train(data):\n",
    "    # Enumerate over the data\n",
    "    for batch in loader:\n",
    "      # Use GPU\n",
    "      batch.to(device)\n",
    "      # Reset gradients\n",
    "      optimizer.zero_grad()\n",
    "      # Passing the node features and the connection info\n",
    "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "      # Calculating the loss and gradients\n",
    "      loss = loss_fn(pred, batch.y)\n",
    "      loss.backward()\n",
    "      # Update using the gradients\n",
    "      optimizer.step()\n",
    "    return loss, embedding\n",
    "\n",
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    loss, h = train(data)\n",
    "    losses.append(loss)\n",
    "    if epoch % 1 == 0:\n",
    "      print(f\"Epoch {epoch} | Train Loss {loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning (training loss)\n",
    "import seaborn as sns\n",
    "losses_float = [float(loss.cpu().detach().numpy()) for loss in losses]\n",
    "loss_indices = [i for i,l in enumerate(losses_float)]\n",
    "plt = sns.lineplot(losses_float)\n",
    "plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Analyze the results for one batch\n",
    "test_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    test_batch.to(device)\n",
    "    pred, embed = model(test_batch.x.float(), test_batch.edge_index, test_batch.batch)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"y_real\"] = test_batch.y.tolist()\n",
    "    df[\"y_pred\"] = pred.tolist()\n",
    "df[\"y_real\"] = df[\"y_real\"].apply(lambda row: row[0])\n",
    "df[\"y_pred\"] = df[\"y_pred\"].apply(lambda row: row[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = sns.scatterplot(data=df, x=\"y_real\", y=\"y_pred\")\n",
    "plt.set(xlim=(-10, 10))\n",
    "plt.set(ylim=(-10, 10))\n",
    "plt.plot([-10, 10], [-10, 10], color='red', linestyle='--')  # Plotting the y=x line\n",
    "plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 (clang-1500.1.0.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
